{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar_input\n",
    "from train import *\n",
    "from loss import *\n",
    "from accuracy import *\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "IMAGE_SIZE = 24\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "max_steps = 20000\n",
    "train_dir = 'cifar10_alexnet_model/'\n",
    "batch_size = 128\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "  \"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "  Add summary for \"Loss\" and \"Loss/avg\".\n",
    "  Args:\n",
    "    logits: Logits from inference().\n",
    "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "\n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  # Calculate the average cross entropy loss across the batch.\n",
    "  # labels = tf.cast(labels, tf.int64)\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "  # decay terms (L2 loss).\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "  \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "\n",
    "  Generates moving average for all losses and associated summaries for\n",
    "  visualizing the performance of the network.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "  Returns:\n",
    "    loss_averages_op: op for generating moving averages of losses.\n",
    "  \"\"\"\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "  losses = tf.get_collection('losses')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "  return loss_averages_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    "  \"\"\"Train CIFAR-10 model.\n",
    "\n",
    "  Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "  Returns:\n",
    "    train_op: op for training.\n",
    "  \"\"\"\n",
    "  # Variables that affect learning rate.\n",
    "  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / 128\n",
    "  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "  # Decay the learning rate exponentially based on the number of steps.\n",
    "  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "\n",
    "  # Generate moving averages of all losses and associated summaries.\n",
    "  loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "  # Compute gradients.\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "  # Apply gradients.\n",
    "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "  variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float16 if 0 else tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float16 if 0 else tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var\n",
    "\n",
    "def inference(images):\n",
    "  \"\"\"Build the CIFAR-10 model.\n",
    "\n",
    "  Args:\n",
    "    images: Images returned from distorted_inputs() or inputs().\n",
    "\n",
    "  Returns:\n",
    "    Logits.\n",
    "  \"\"\"\n",
    "  # We instantiate all variables using tf.get_variable() instead of\n",
    "  # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "  # If we only ran this model on a single GPU, we could simplify this function\n",
    "  # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "  #\n",
    "  # conv1\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 3, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    #_activation_summary(conv1)\n",
    "\n",
    "  # pool1\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "  # norm1\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "  # conv2\n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 64, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    #_activation_summary(conv2)\n",
    "\n",
    "  # norm2\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "  # pool2\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "  # local3\n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    # Move everything into depth so we can perform a single matrix multiply.\n",
    "    reshape = tf.reshape(pool2, [128, -1])\n",
    "    dim = reshape.get_shape()[1].value\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    #_activation_summary(local3)\n",
    "\n",
    "  # local4\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    #_activation_summary(local4)\n",
    "\n",
    "  # linear layer(WX + b),\n",
    "  # We don't apply softmax here because \n",
    "  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits \n",
    "  # and performs the softmax internally for efficiency.\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                              tf.constant_initializer(0.0))\n",
    "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    #_activation_summary(softmax_linear)\n",
    "\n",
    "  return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 2 into cifar10_alexnet_model/model.ckpt.\n",
      "2017-03-11 16:00:19.423482: step 0, loss = 6.42, accuracy = 0.09 (60.4 examples/sec; 2.120 sec/batch)\n",
      "2017-03-11 16:00:21.284629: step 10, loss = 6.28, accuracy = 0.20 (552.7 examples/sec; 0.232 sec/batch)\n",
      "2017-03-11 16:00:23.379771: step 20, loss = 6.19, accuracy = 0.21 (584.3 examples/sec; 0.219 sec/batch)\n",
      "2017-03-11 16:00:25.487247: step 30, loss = 6.06, accuracy = 0.27 (585.0 examples/sec; 0.219 sec/batch)\n",
      "2017-03-11 16:00:27.609575: step 40, loss = 5.88, accuracy = 0.25 (599.0 examples/sec; 0.214 sec/batch)\n",
      "2017-03-11 16:00:29.709609: step 50, loss = 5.84, accuracy = 0.28 (606.4 examples/sec; 0.211 sec/batch)\n",
      "2017-03-11 16:00:31.813589: step 60, loss = 5.88, accuracy = 0.29 (599.6 examples/sec; 0.213 sec/batch)\n",
      "2017-03-11 16:00:33.915883: step 70, loss = 6.04, accuracy = 0.30 (627.1 examples/sec; 0.204 sec/batch)\n",
      "2017-03-11 16:00:36.043457: step 80, loss = 5.73, accuracy = 0.27 (614.0 examples/sec; 0.208 sec/batch)\n",
      "2017-03-11 16:00:38.154370: step 90, loss = 5.61, accuracy = 0.32 (589.2 examples/sec; 0.217 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.76313\n",
      "2017-03-11 16:00:40.266377: step 100, loss = 5.53, accuracy = 0.36 (601.4 examples/sec; 0.213 sec/batch)\n",
      "2017-03-11 16:00:42.368879: step 110, loss = 5.47, accuracy = 0.38 (632.0 examples/sec; 0.203 sec/batch)\n",
      "2017-03-11 16:00:44.502965: step 120, loss = 5.40, accuracy = 0.44 (599.0 examples/sec; 0.214 sec/batch)\n",
      "2017-03-11 16:00:46.614039: step 130, loss = 5.38, accuracy = 0.38 (587.9 examples/sec; 0.218 sec/batch)\n",
      "2017-03-11 16:00:48.732894: step 140, loss = 5.67, accuracy = 0.21 (621.1 examples/sec; 0.206 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 145 into cifar10_alexnet_model/model.ckpt.\n",
      "2017-03-11 16:00:50.867511: step 150, loss = 5.54, accuracy = 0.32 (582.8 examples/sec; 0.220 sec/batch)\n",
      "2017-03-11 16:00:52.970840: step 160, loss = 5.67, accuracy = 0.27 (640.7 examples/sec; 0.200 sec/batch)\n",
      "2017-03-11 16:00:55.093931: step 170, loss = 5.29, accuracy = 0.32 (604.0 examples/sec; 0.212 sec/batch)\n",
      "2017-03-11 16:00:57.200933: step 180, loss = 5.41, accuracy = 0.34 (657.9 examples/sec; 0.195 sec/batch)\n",
      "2017-03-11 16:00:59.317811: step 190, loss = 5.31, accuracy = 0.37 (615.4 examples/sec; 0.208 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.72278\n",
      "2017-03-11 16:01:01.439569: step 200, loss = 5.07, accuracy = 0.48 (601.0 examples/sec; 0.213 sec/batch)\n",
      "2017-03-11 16:01:03.546076: step 210, loss = 5.14, accuracy = 0.44 (649.4 examples/sec; 0.197 sec/batch)\n",
      "2017-03-11 16:01:05.670722: step 220, loss = 5.19, accuracy = 0.34 (595.0 examples/sec; 0.215 sec/batch)\n",
      "2017-03-11 16:01:07.793470: step 230, loss = 5.05, accuracy = 0.45 (599.7 examples/sec; 0.213 sec/batch)\n",
      "2017-03-11 16:01:09.896075: step 240, loss = 4.79, accuracy = 0.47 (641.0 examples/sec; 0.200 sec/batch)\n",
      "2017-03-11 16:01:12.001780: step 250, loss = 4.87, accuracy = 0.45 (602.9 examples/sec; 0.212 sec/batch)\n",
      "2017-03-11 16:01:14.116683: step 260, loss = 5.06, accuracy = 0.38 (617.9 examples/sec; 0.207 sec/batch)\n",
      "2017-03-11 16:01:16.236379: step 270, loss = 4.91, accuracy = 0.41 (634.9 examples/sec; 0.202 sec/batch)\n",
      "2017-03-11 16:01:18.366338: step 280, loss = 4.96, accuracy = 0.39 (571.9 examples/sec; 0.224 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 287 into cifar10_alexnet_model/model.ckpt.\n",
      "2017-03-11 16:01:20.502673: step 290, loss = 5.07, accuracy = 0.32 (584.6 examples/sec; 0.219 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.72685\n",
      "2017-03-11 16:01:22.594983: step 300, loss = 4.75, accuracy = 0.47 (616.4 examples/sec; 0.208 sec/batch)\n",
      "2017-03-11 16:01:24.719354: step 310, loss = 4.75, accuracy = 0.38 (608.5 examples/sec; 0.210 sec/batch)\n",
      "2017-03-11 16:01:26.836215: step 320, loss = 4.48, accuracy = 0.59 (610.3 examples/sec; 0.210 sec/batch)\n",
      "2017-03-11 16:01:28.952245: step 330, loss = 4.87, accuracy = 0.38 (600.1 examples/sec; 0.213 sec/batch)\n",
      "2017-03-11 16:01:31.073526: step 340, loss = 4.70, accuracy = 0.48 (611.9 examples/sec; 0.209 sec/batch)\n",
      "2017-03-11 16:01:33.189436: step 350, loss = 4.56, accuracy = 0.49 (603.3 examples/sec; 0.212 sec/batch)\n",
      "2017-03-11 16:01:35.299853: step 360, loss = 4.52, accuracy = 0.45 (601.7 examples/sec; 0.213 sec/batch)\n",
      "2017-03-11 16:01:37.420175: step 370, loss = 4.42, accuracy = 0.55 (597.3 examples/sec; 0.214 sec/batch)\n",
      "2017-03-11 16:01:39.542886: step 380, loss = 4.52, accuracy = 0.45 (604.5 examples/sec; 0.212 sec/batch)\n",
      "2017-03-11 16:01:41.648164: step 390, loss = 4.49, accuracy = 0.41 (611.8 examples/sec; 0.209 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.72082\n",
      "2017-03-11 16:01:43.777996: step 400, loss = 4.42, accuracy = 0.44 (593.8 examples/sec; 0.216 sec/batch)\n",
      "2017-03-11 16:01:45.909797: step 410, loss = 4.43, accuracy = 0.43 (617.5 examples/sec; 0.207 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar_input.build_input('cifar10', '../../cifar/cifar10/data_batch*', 128, 'train')\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    losses = loss(logits, labels)\n",
    "    \n",
    "    accuracies = accuracy(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = train(losses, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.makedirs(train_dir)\n",
    "        open(train_dir+'training_data.csv', 'w').close()\n",
    "        f = open(train_dir+\"log.txt\",'ab')\n",
    "        f.write('\\n\\n==== Run ===\\nInfo: Alexnet\\n')\n",
    "        f.close()\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        self._start_time = time.time()\n",
    "        return tf.train.SessionRunArgs([losses, accuracies])  # Asks for loss value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        duration = time.time() - self._start_time\n",
    "        loss_value = run_values.results[0]\n",
    "        accuracy_value = run_values.results[1]\n",
    "        if self._step % 10 == 0:\n",
    "          num_examples_per_step = batch_size\n",
    "          examples_per_sec = num_examples_per_step / duration\n",
    "          sec_per_batch = float(duration)\n",
    "\n",
    "          format_str = ('%s: step %d, loss = %.2f, accuracy = %.2f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, loss_value, accuracy_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "          f = open(train_dir+\"log.txt\",'ab')\n",
    "          f.write('{0}: step {1}, loss = {2:.4f} ({3:.2f} examples/sec; {4:.2f} sec/batch)\\n'.format(datetime.now(), self._step, loss_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "          f.close()\n",
    "          f = open(train_dir+\"training_data.csv\",'ab')\n",
    "          f.write('{0},{1},{2}\\n'.format(self._step,loss_value,accuracy_value))\n",
    "          f.close()\n",
    "            \n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=train_dir,\n",
    "                                           hooks=[tf.train.StopAtStepHook(last_step=max_steps),\n",
    "                                                  tf.train.NanTensorHook(losses),\n",
    "                                                  _LoggerHook()],save_checkpoint_secs=30, \n",
    "                                           config=tf.ConfigProto(\n",
    "                                               log_device_placement=log_device_placement)) as mon_sess:\n",
    "        while not mon_sess.should_stop():\n",
    "            mon_sess.run(train_op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
