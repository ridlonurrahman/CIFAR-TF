{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10, cifar_input, cifar_input_ver2\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "'''tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 25000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")'''\n",
    "\n",
    "IMAGE_SIZE = 24\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 100\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distorted_inputs():\n",
    "  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If no data_dir\n",
    "  \"\"\"\n",
    "  data_dir = '/tmp/cifar10_data'\n",
    "  if not data_dir:\n",
    "    raise ValueError('Please supply a data_dir')\n",
    "  data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "  images, labels = cifar_input_ver2.distorted_inputs(data_dir=data_dir,\n",
    "                                                  batch_size=128)\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "images, labels = cifar_input.build_input('cifar10', 'cifar10/data_batch*', 128, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images, labels = distorted_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = inference(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "  \"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "  Add summary for \"Loss\" and \"Loss/avg\".\n",
    "  Args:\n",
    "    logits: Logits from inference().\n",
    "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "\n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  # Calculate the average cross entropy loss across the batch.\n",
    "  # labels = tf.cast(labels, tf.int64)\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "  # decay terms (L2 loss).\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = loss(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "  \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "\n",
    "  Generates moving average for all losses and associated summaries for\n",
    "  visualizing the performance of the network.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "  Returns:\n",
    "    loss_averages_op: op for generating moving averages of losses.\n",
    "  \"\"\"\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "  losses = tf.get_collection('losses')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "  return loss_averages_op\n",
    "\n",
    "def train(total_loss, global_step):\n",
    "  \"\"\"Train CIFAR-10 model.\n",
    "\n",
    "  Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "  Returns:\n",
    "    train_op: op for training.\n",
    "  \"\"\"\n",
    "  # Variables that affect learning rate.\n",
    "  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / 128\n",
    "  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "  # Decay the learning rate exponentially based on the number of steps.\n",
    "  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "\n",
    "  # Generate moving averages of all losses and associated summaries.\n",
    "  loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "  # Compute gradients.\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "  # Apply gradients.\n",
    "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "  variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_op = train(loss, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/cifar10_train/model.ckpt.\n",
      "2017-03-08 03:17:41.949434: step 0, loss = 8.69 (116.9 examples/sec; 1.095 sec/batch)\n",
      "2017-03-08 03:17:50.410190: step 50, loss = 8.29 (716.1 examples/sec; 0.179 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 5.83009\n",
      "2017-03-08 03:17:58.954772: step 100, loss = 7.92 (719.3 examples/sec; 0.178 sec/batch)\n",
      "2017-03-08 03:18:07.448215: step 150, loss = 7.77 (778.5 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 5.89956\n",
      "2017-03-08 03:18:15.906031: step 200, loss = 7.52 (716.7 examples/sec; 0.179 sec/batch)\n",
      "2017-03-08 03:18:24.302474: step 250, loss = 7.10 (760.0 examples/sec; 0.168 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 5.96981\n",
      "2017-03-08 03:18:32.655773: step 300, loss = 7.06 (779.3 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:18:40.846047: step 350, loss = 6.83 (782.3 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.10037\n",
      "2017-03-08 03:18:49.048666: step 400, loss = 6.70 (762.6 examples/sec; 0.168 sec/batch)\n",
      "2017-03-08 03:18:57.189541: step 450, loss = 6.58 (817.5 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.13771\n",
      "2017-03-08 03:19:05.341127: step 500, loss = 6.46 (757.8 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:19:13.513446: step 550, loss = 6.10 (782.2 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.12264\n",
      "2017-03-08 03:19:21.674193: step 600, loss = 5.59 (769.7 examples/sec; 0.166 sec/batch)\n",
      "2017-03-08 03:19:29.843362: step 650, loss = 5.90 (804.5 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.11833\n",
      "2017-03-08 03:19:38.017603: step 700, loss = 5.69 (734.2 examples/sec; 0.174 sec/batch)\n",
      "2017-03-08 03:19:46.169982: step 750, loss = 5.40 (768.7 examples/sec; 0.167 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.13203\n",
      "2017-03-08 03:19:54.327813: step 800, loss = 5.59 (749.4 examples/sec; 0.171 sec/batch)\n",
      "2017-03-08 03:20:02.488501: step 850, loss = 5.37 (763.0 examples/sec; 0.168 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.12342\n",
      "2017-03-08 03:20:10.658374: step 900, loss = 5.04 (753.6 examples/sec; 0.170 sec/batch)\n",
      "2017-03-08 03:20:18.841350: step 950, loss = 5.11 (746.6 examples/sec; 0.171 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.12457\n",
      "2017-03-08 03:20:26.985653: step 1000, loss = 4.79 (738.6 examples/sec; 0.173 sec/batch)\n",
      "2017-03-08 03:20:35.130089: step 1050, loss = 4.86 (818.3 examples/sec; 0.156 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.14194\n",
      "2017-03-08 03:20:43.265625: step 1100, loss = 4.91 (741.3 examples/sec; 0.173 sec/batch)\n",
      "2017-03-08 03:20:51.396362: step 1150, loss = 4.68 (748.5 examples/sec; 0.171 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.15817\n",
      "2017-03-08 03:20:59.504736: step 1200, loss = 4.68 (750.9 examples/sec; 0.170 sec/batch)\n",
      "2017-03-08 03:21:07.625555: step 1250, loss = 4.36 (780.8 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.16584\n",
      "2017-03-08 03:21:15.722875: step 1300, loss = 4.05 (824.7 examples/sec; 0.155 sec/batch)\n",
      "2017-03-08 03:21:23.806521: step 1350, loss = 4.28 (788.5 examples/sec; 0.162 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.18704\n",
      "2017-03-08 03:21:31.885573: step 1400, loss = 4.52 (778.2 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:21:39.948785: step 1450, loss = 4.36 (774.0 examples/sec; 0.165 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.19835\n",
      "2017-03-08 03:21:48.018702: step 1500, loss = 4.25 (781.6 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:21:56.069540: step 1550, loss = 3.93 (781.3 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.20775\n",
      "2017-03-08 03:22:04.129932: step 1600, loss = 4.01 (792.1 examples/sec; 0.162 sec/batch)\n",
      "2017-03-08 03:22:12.163292: step 1650, loss = 3.73 (788.8 examples/sec; 0.162 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.23267\n",
      "2017-03-08 03:22:20.172542: step 1700, loss = 3.82 (784.0 examples/sec; 0.163 sec/batch)\n",
      "2017-03-08 03:22:28.198999: step 1750, loss = 3.91 (806.4 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.23156\n",
      "2017-03-08 03:22:36.220146: step 1800, loss = 3.99 (759.8 examples/sec; 0.168 sec/batch)\n",
      "2017-03-08 03:22:44.258810: step 1850, loss = 3.78 (834.1 examples/sec; 0.153 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.21612\n",
      "2017-03-08 03:22:52.308265: step 1900, loss = 3.48 (805.3 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:23:00.322251: step 1950, loss = 3.67 (751.2 examples/sec; 0.170 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.23137\n",
      "2017-03-08 03:23:08.354456: step 2000, loss = 3.60 (767.4 examples/sec; 0.167 sec/batch)\n",
      "2017-03-08 03:23:16.380132: step 2050, loss = 3.20 (738.9 examples/sec; 0.173 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.2323\n",
      "2017-03-08 03:23:24.400313: step 2100, loss = 3.64 (749.0 examples/sec; 0.171 sec/batch)\n",
      "2017-03-08 03:23:32.413425: step 2150, loss = 3.14 (829.1 examples/sec; 0.154 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.23521\n",
      "2017-03-08 03:23:40.438342: step 2200, loss = 3.47 (806.2 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:23:48.413274: step 2250, loss = 3.30 (775.5 examples/sec; 0.165 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.27112\n",
      "2017-03-08 03:23:56.384128: step 2300, loss = 3.49 (801.0 examples/sec; 0.160 sec/batch)\n",
      "2017-03-08 03:24:04.342631: step 2350, loss = 3.52 (779.9 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.28053\n",
      "2017-03-08 03:24:12.308328: step 2400, loss = 3.24 (794.5 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:24:20.280549: step 2450, loss = 2.90 (768.0 examples/sec; 0.167 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.26531\n",
      "2017-03-08 03:24:28.268796: step 2500, loss = 2.91 (775.1 examples/sec; 0.165 sec/batch)\n",
      "2017-03-08 03:24:36.249283: step 2550, loss = 2.92 (808.6 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.26082\n",
      "2017-03-08 03:24:44.239424: step 2600, loss = 3.17 (808.9 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:24:52.220494: step 2650, loss = 3.12 (763.4 examples/sec; 0.168 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.27737\n",
      "2017-03-08 03:25:00.171604: step 2700, loss = 3.08 (742.7 examples/sec; 0.172 sec/batch)\n",
      "2017-03-08 03:25:08.125584: step 2750, loss = 3.07 (842.8 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.26054\n",
      "2017-03-08 03:25:16.144459: step 2800, loss = 2.87 (760.3 examples/sec; 0.168 sec/batch)\n",
      "2017-03-08 03:25:24.100351: step 2850, loss = 2.80 (840.3 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.27861\n",
      "2017-03-08 03:25:32.070090: step 2900, loss = 3.02 (851.4 examples/sec; 0.150 sec/batch)\n",
      "2017-03-08 03:25:40.035356: step 2950, loss = 2.79 (762.1 examples/sec; 0.168 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.27402\n",
      "2017-03-08 03:25:48.011384: step 3000, loss = 2.78 (777.5 examples/sec; 0.165 sec/batch)\n",
      "2017-03-08 03:25:55.968696: step 3050, loss = 2.82 (800.8 examples/sec; 0.160 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.26821\n",
      "2017-03-08 03:26:03.962284: step 3100, loss = 3.03 (803.6 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:26:11.940555: step 3150, loss = 2.53 (806.8 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.2743\n",
      "2017-03-08 03:26:19.902418: step 3200, loss = 2.97 (759.0 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:26:27.861260: step 3250, loss = 2.89 (819.4 examples/sec; 0.156 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.26287\n",
      "2017-03-08 03:26:35.867838: step 3300, loss = 2.64 (758.5 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:26:43.799333: step 3350, loss = 2.51 (829.1 examples/sec; 0.154 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3177\n",
      "2017-03-08 03:26:51.697486: step 3400, loss = 2.73 (823.4 examples/sec; 0.155 sec/batch)\n",
      "2017-03-08 03:26:59.569437: step 3450, loss = 2.53 (808.2 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.33754\n",
      "2017-03-08 03:27:07.475099: step 3500, loss = 2.49 (774.4 examples/sec; 0.165 sec/batch)\n",
      "2017-03-08 03:27:15.368850: step 3550, loss = 2.58 (841.6 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.32531\n",
      "2017-03-08 03:27:23.284750: step 3600, loss = 2.80 (775.2 examples/sec; 0.165 sec/batch)\n",
      "2017-03-08 03:27:31.162907: step 3650, loss = 2.25 (844.4 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.34447\n",
      "2017-03-08 03:27:39.048525: step 3700, loss = 2.50 (795.5 examples/sec; 0.161 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 3719 into /tmp/cifar10_train/model.ckpt.\n",
      "2017-03-08 03:27:46.962814: step 3750, loss = 2.77 (811.7 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.34256\n",
      "2017-03-08 03:27:54.813073: step 3800, loss = 2.67 (818.8 examples/sec; 0.156 sec/batch)\n",
      "2017-03-08 03:28:02.689263: step 3850, loss = 2.85 (838.1 examples/sec; 0.153 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35715\n",
      "2017-03-08 03:28:10.542974: step 3900, loss = 2.47 (772.8 examples/sec; 0.166 sec/batch)\n",
      "2017-03-08 03:28:18.412969: step 3950, loss = 2.67 (817.8 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.34584\n",
      "2017-03-08 03:28:26.302113: step 4000, loss = 2.50 (778.3 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:28:34.150309: step 4050, loss = 2.63 (838.2 examples/sec; 0.153 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35577\n",
      "2017-03-08 03:28:42.035055: step 4100, loss = 2.30 (807.9 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:28:49.888496: step 4150, loss = 2.65 (780.6 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.36402\n",
      "2017-03-08 03:28:57.750642: step 4200, loss = 2.26 (795.4 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:29:05.617674: step 4250, loss = 2.54 (845.5 examples/sec; 0.151 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.34416\n",
      "2017-03-08 03:29:13.512246: step 4300, loss = 2.54 (784.9 examples/sec; 0.163 sec/batch)\n",
      "2017-03-08 03:29:21.363971: step 4350, loss = 2.39 (777.3 examples/sec; 0.165 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35933\n",
      "2017-03-08 03:29:29.235707: step 4400, loss = 2.28 (808.6 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:29:37.094831: step 4450, loss = 2.66 (808.8 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.36448\n",
      "2017-03-08 03:29:44.947990: step 4500, loss = 2.20 (788.4 examples/sec; 0.162 sec/batch)\n",
      "2017-03-08 03:29:52.843034: step 4550, loss = 2.27 (759.5 examples/sec; 0.169 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.349\n",
      "2017-03-08 03:30:00.698712: step 4600, loss = 2.57 (789.2 examples/sec; 0.162 sec/batch)\n",
      "2017-03-08 03:30:08.582070: step 4650, loss = 2.35 (858.2 examples/sec; 0.149 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.34789\n",
      "2017-03-08 03:30:16.452696: step 4700, loss = 2.56 (797.5 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:30:24.310993: step 4750, loss = 2.11 (815.2 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35817\n",
      "2017-03-08 03:30:32.180119: step 4800, loss = 1.95 (786.3 examples/sec; 0.163 sec/batch)\n",
      "2017-03-08 03:30:40.037910: step 4850, loss = 2.48 (816.5 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.34619\n",
      "2017-03-08 03:30:47.938626: step 4900, loss = 2.26 (757.0 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:30:55.796417: step 4950, loss = 2.32 (807.4 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35495\n",
      "2017-03-08 03:31:03.672728: step 5000, loss = 2.16 (804.7 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:31:11.547845: step 5050, loss = 2.39 (784.5 examples/sec; 0.163 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3506\n",
      "2017-03-08 03:31:19.421663: step 5100, loss = 2.08 (792.7 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:31:27.290659: step 5150, loss = 2.18 (807.2 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35946\n",
      "2017-03-08 03:31:35.144412: step 5200, loss = 2.75 (815.3 examples/sec; 0.157 sec/batch)\n",
      "2017-03-08 03:31:43.005205: step 5250, loss = 2.36 (869.0 examples/sec; 0.147 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35967\n",
      "2017-03-08 03:31:50.869979: step 5300, loss = 2.32 (757.4 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:31:58.715020: step 5350, loss = 2.14 (798.1 examples/sec; 0.160 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.36878\n",
      "2017-03-08 03:32:06.569667: step 5400, loss = 2.29 (791.8 examples/sec; 0.162 sec/batch)\n",
      "2017-03-08 03:32:14.418236: step 5450, loss = 2.27 (819.1 examples/sec; 0.156 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.36729\n",
      "2017-03-08 03:32:22.275067: step 5500, loss = 2.45 (840.7 examples/sec; 0.152 sec/batch)\n",
      "2017-03-08 03:32:30.126034: step 5550, loss = 2.26 (800.7 examples/sec; 0.160 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3626\n",
      "2017-03-08 03:32:37.992148: step 5600, loss = 2.38 (807.2 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:32:45.845848: step 5650, loss = 2.17 (836.1 examples/sec; 0.153 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.35211\n",
      "2017-03-08 03:32:53.735828: step 5700, loss = 2.20 (778.3 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:33:01.573875: step 5750, loss = 2.30 (809.1 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.37245\n",
      "2017-03-08 03:33:09.427232: step 5800, loss = 2.35 (818.5 examples/sec; 0.156 sec/batch)\n",
      "2017-03-08 03:33:17.247110: step 5850, loss = 2.33 (782.5 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3876\n",
      "2017-03-08 03:33:25.084306: step 5900, loss = 2.24 (812.1 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:33:32.921266: step 5950, loss = 2.16 (850.6 examples/sec; 0.150 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38165\n",
      "2017-03-08 03:33:40.754300: step 6000, loss = 2.35 (809.9 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:33:48.609291: step 6050, loss = 2.45 (817.1 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.37577\n",
      "2017-03-08 03:33:56.437031: step 6100, loss = 2.16 (785.5 examples/sec; 0.163 sec/batch)\n",
      "2017-03-08 03:34:04.263113: step 6150, loss = 2.07 (800.3 examples/sec; 0.160 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38631\n",
      "2017-03-08 03:34:12.095665: step 6200, loss = 2.14 (819.2 examples/sec; 0.156 sec/batch)\n",
      "2017-03-08 03:34:19.912245: step 6250, loss = 2.37 (854.6 examples/sec; 0.150 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.40351\n",
      "2017-03-08 03:34:27.711393: step 6300, loss = 2.20 (806.5 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:34:35.548861: step 6350, loss = 2.30 (778.9 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38255\n",
      "2017-03-08 03:34:43.381688: step 6400, loss = 2.04 (781.6 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:34:51.181254: step 6450, loss = 2.24 (823.6 examples/sec; 0.155 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3898\n",
      "2017-03-08 03:34:59.032078: step 6500, loss = 2.02 (772.3 examples/sec; 0.166 sec/batch)\n",
      "2017-03-08 03:35:06.854347: step 6550, loss = 2.18 (813.0 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38655\n",
      "2017-03-08 03:35:14.689443: step 6600, loss = 2.25 (755.3 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:35:22.487216: step 6650, loss = 2.16 (833.7 examples/sec; 0.154 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.40239\n",
      "2017-03-08 03:35:30.307103: step 6700, loss = 2.02 (807.3 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:35:38.142493: step 6750, loss = 2.01 (818.1 examples/sec; 0.156 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38088\n",
      "2017-03-08 03:35:45.980484: step 6800, loss = 2.39 (796.0 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:35:53.820339: step 6850, loss = 2.16 (813.8 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.37914\n",
      "2017-03-08 03:36:01.654910: step 6900, loss = 2.07 (782.8 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:36:09.477188: step 6950, loss = 2.07 (812.8 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3965\n",
      "2017-03-08 03:36:17.289388: step 7000, loss = 2.19 (811.4 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:36:25.102433: step 7050, loss = 2.08 (843.3 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3891\n",
      "2017-03-08 03:36:32.942039: step 7100, loss = 2.02 (828.7 examples/sec; 0.154 sec/batch)\n",
      "2017-03-08 03:36:40.774899: step 7150, loss = 2.29 (805.1 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38239\n",
      "2017-03-08 03:36:48.609641: step 7200, loss = 2.21 (805.0 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:36:56.424741: step 7250, loss = 2.10 (815.5 examples/sec; 0.157 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38617\n",
      "2017-03-08 03:37:04.266670: step 7300, loss = 2.43 (823.2 examples/sec; 0.155 sec/batch)\n",
      "2017-03-08 03:37:12.084986: step 7350, loss = 2.46 (853.2 examples/sec; 0.150 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39738\n",
      "2017-03-08 03:37:19.899717: step 7400, loss = 2.14 (795.8 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:37:27.726478: step 7450, loss = 1.98 (805.9 examples/sec; 0.159 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3858\n",
      "2017-03-08 03:37:35.558006: step 7500, loss = 1.90 (773.9 examples/sec; 0.165 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 7542 into /tmp/cifar10_train/model.ckpt.\n",
      "2017-03-08 03:37:43.520279: step 7550, loss = 2.10 (851.9 examples/sec; 0.150 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.33152\n",
      "2017-03-08 03:37:51.351908: step 7600, loss = 2.20 (816.3 examples/sec; 0.157 sec/batch)\n",
      "2017-03-08 03:37:59.175744: step 7650, loss = 2.12 (786.9 examples/sec; 0.163 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3859\n",
      "2017-03-08 03:38:07.011897: step 7700, loss = 2.40 (796.0 examples/sec; 0.161 sec/batch)\n",
      "2017-03-08 03:38:14.833735: step 7750, loss = 2.11 (810.7 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38984\n",
      "2017-03-08 03:38:22.661093: step 7800, loss = 2.20 (778.7 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:38:30.479378: step 7850, loss = 2.38 (769.2 examples/sec; 0.166 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39549\n",
      "2017-03-08 03:38:38.298313: step 7900, loss = 1.75 (810.1 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:38:46.100564: step 7950, loss = 2.32 (788.7 examples/sec; 0.162 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38572\n",
      "2017-03-08 03:38:53.958429: step 8000, loss = 2.23 (813.1 examples/sec; 0.157 sec/batch)\n",
      "2017-03-08 03:39:01.770826: step 8050, loss = 2.28 (849.6 examples/sec; 0.151 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39423\n",
      "2017-03-08 03:39:09.597449: step 8100, loss = 2.15 (818.2 examples/sec; 0.156 sec/batch)\n",
      "2017-03-08 03:39:17.438081: step 8150, loss = 2.05 (796.0 examples/sec; 0.161 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.375\n",
      "2017-03-08 03:39:25.283079: step 8200, loss = 2.30 (787.6 examples/sec; 0.163 sec/batch)\n",
      "2017-03-08 03:39:33.111504: step 8250, loss = 2.14 (781.7 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38477\n",
      "2017-03-08 03:39:40.944441: step 8300, loss = 2.19 (786.6 examples/sec; 0.163 sec/batch)\n",
      "2017-03-08 03:39:48.774188: step 8350, loss = 1.88 (780.9 examples/sec; 0.164 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39107\n",
      "2017-03-08 03:39:56.591826: step 8400, loss = 2.36 (804.3 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:40:04.439176: step 8450, loss = 1.96 (795.8 examples/sec; 0.161 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3827\n",
      "2017-03-08 03:40:12.259498: step 8500, loss = 2.14 (775.1 examples/sec; 0.165 sec/batch)\n",
      "2017-03-08 03:40:20.116098: step 8550, loss = 2.12 (858.9 examples/sec; 0.149 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.384\n",
      "2017-03-08 03:40:27.923172: step 8600, loss = 1.90 (756.2 examples/sec; 0.169 sec/batch)\n",
      "2017-03-08 03:40:35.743307: step 8650, loss = 2.25 (808.1 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38242\n",
      "2017-03-08 03:40:43.591539: step 8700, loss = 2.18 (826.7 examples/sec; 0.155 sec/batch)\n",
      "2017-03-08 03:40:51.399026: step 8750, loss = 1.99 (839.9 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39858\n",
      "2017-03-08 03:40:59.219945: step 8800, loss = 2.07 (791.7 examples/sec; 0.162 sec/batch)\n",
      "2017-03-08 03:41:07.060693: step 8850, loss = 2.28 (797.0 examples/sec; 0.161 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.37687\n",
      "2017-03-08 03:41:14.901562: step 8900, loss = 2.15 (781.0 examples/sec; 0.164 sec/batch)\n",
      "2017-03-08 03:41:22.701534: step 8950, loss = 2.05 (823.4 examples/sec; 0.155 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.3984\n",
      "2017-03-08 03:41:30.530237: step 9000, loss = 1.99 (808.7 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:41:38.363444: step 9050, loss = 2.13 (851.7 examples/sec; 0.150 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38042\n",
      "2017-03-08 03:41:46.205088: step 9100, loss = 2.16 (761.8 examples/sec; 0.168 sec/batch)\n",
      "2017-03-08 03:41:54.035199: step 9150, loss = 2.21 (763.0 examples/sec; 0.168 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39529\n",
      "2017-03-08 03:42:01.839818: step 9200, loss = 1.98 (832.2 examples/sec; 0.154 sec/batch)\n",
      "2017-03-08 03:42:09.690326: step 9250, loss = 2.44 (775.9 examples/sec; 0.165 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.37842\n",
      "2017-03-08 03:42:17.517471: step 9300, loss = 2.20 (832.8 examples/sec; 0.154 sec/batch)\n",
      "2017-03-08 03:42:25.339276: step 9350, loss = 1.87 (801.7 examples/sec; 0.160 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39525\n",
      "2017-03-08 03:42:33.156342: step 9400, loss = 2.33 (811.4 examples/sec; 0.158 sec/batch)\n",
      "2017-03-08 03:42:40.976110: step 9450, loss = 2.15 (821.1 examples/sec; 0.156 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.39201\n",
      "2017-03-08 03:42:48.798838: step 9500, loss = 1.98 (822.4 examples/sec; 0.156 sec/batch)\n",
      "2017-03-08 03:42:56.626996: step 9550, loss = 2.09 (788.5 examples/sec; 0.162 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38977\n",
      "2017-03-08 03:43:04.449103: step 9600, loss = 2.22 (807.5 examples/sec; 0.159 sec/batch)\n",
      "2017-03-08 03:43:12.230783: step 9650, loss = 1.75 (842.8 examples/sec; 0.152 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.4022\n",
      "2017-03-08 03:43:20.068160: step 9700, loss = 2.30 (790.4 examples/sec; 0.162 sec/batch)\n",
      "2017-03-08 03:43:27.911481: step 9750, loss = 1.87 (855.9 examples/sec; 0.150 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38088\n",
      "2017-03-08 03:43:35.741533: step 9800, loss = 2.09 (770.3 examples/sec; 0.166 sec/batch)\n",
      "2017-03-08 03:43:43.581363: step 9850, loss = 2.08 (824.2 examples/sec; 0.155 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 6.38985\n",
      "2017-03-08 03:43:51.391680: step 9900, loss = 2.16 (818.8 examples/sec; 0.156 sec/batch)\n",
      "2017-03-08 03:43:59.217687: step 9950, loss = 2.11 (809.6 examples/sec; 0.158 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/cifar10_train/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "max_steps = 10000\n",
    "train_dir = '/tmp/cifar10_train'\n",
    "batch_size = 128\n",
    "log_device_placement = False\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar_input.build_input('cifar100', 'cifar100/train.bin', 128, 'train')\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = train(loss, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        self._start_time = time.time()\n",
    "        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        duration = time.time() - self._start_time\n",
    "        loss_value = run_values.results\n",
    "        if self._step % 50 == 0:\n",
    "          num_examples_per_step = batch_size\n",
    "          examples_per_sec = num_examples_per_step / duration\n",
    "          sec_per_batch = float(duration)\n",
    "\n",
    "          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, loss_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "            \n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=train_dir,\n",
    "                                           hooks=[tf.train.StopAtStepHook(last_step=max_steps),\n",
    "                                                  tf.train.NanTensorHook(loss),\n",
    "                                                  _LoggerHook()],\n",
    "                                           config=tf.ConfigProto(\n",
    "                                               log_device_placement=log_device_placement)) as mon_sess:\n",
    "        while not mon_sess.should_stop():\n",
    "            mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float16 if 0 else tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float16 if 0 else tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var\n",
    "\n",
    "def inference(images):\n",
    "  \"\"\"Build the CIFAR-10 model.\n",
    "\n",
    "  Args:\n",
    "    images: Images returned from distorted_inputs() or inputs().\n",
    "\n",
    "  Returns:\n",
    "    Logits.\n",
    "  \"\"\"\n",
    "  # We instantiate all variables using tf.get_variable() instead of\n",
    "  # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "  # If we only ran this model on a single GPU, we could simplify this function\n",
    "  # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "  #\n",
    "  # conv1\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 3, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    #_activation_summary(conv1)\n",
    "\n",
    "  # pool1\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "  # norm1\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "  # conv2\n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 64, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    #_activation_summary(conv2)\n",
    "\n",
    "  # norm2\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "  # pool2\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "  # local3\n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    # Move everything into depth so we can perform a single matrix multiply.\n",
    "    reshape = tf.reshape(pool2, [128, -1])\n",
    "    dim = reshape.get_shape()[1].value\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    #_activation_summary(local3)\n",
    "\n",
    "  # local4\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    #_activation_summary(local4)\n",
    "\n",
    "  # linear layer(WX + b),\n",
    "  # We don't apply softmax here because \n",
    "  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits \n",
    "  # and performs the softmax internally for efficiency.\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                              tf.constant_initializer(0.0))\n",
    "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    #_activation_summary(softmax_linear)\n",
    "\n",
    "  return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "def evaluate():\n",
    "  eval_batch_count = 50\n",
    "  \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default() as g:\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar_input.build_input('cifar100', 'cifar100/test.bin', 128, 'eval')\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "    saver = tf.train.Saver()\n",
    "    #summary_writer = tf.summary.FileWriter(FLAGS.eval_dir)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    tf.train.start_queue_runners(sess)\n",
    "\n",
    "    best_precision = 0.0\n",
    "    while True:\n",
    "      try:\n",
    "        ckpt_state = tf.train.get_checkpoint_state(train_dir)\n",
    "      except tf.errors.OutOfRangeError as e:\n",
    "        tf.logging.error('Cannot restore checkpoint: %s', e)\n",
    "        continue\n",
    "      if not (ckpt_state and ckpt_state.model_checkpoint_path):\n",
    "        tf.logging.info('No model to eval yet at %s', train_dir)\n",
    "        continue\n",
    "      tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\n",
    "      saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "\n",
    "      total_prediction, correct_prediction = 0, 0\n",
    "      for _ in six.moves.range(eval_batch_count):\n",
    "        (predictions, truth) = sess.run(\n",
    "            [logits, labels])\n",
    "\n",
    "        truth = np.argmax(truth, axis=1)\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        correct_prediction += np.sum(truth == predictions)\n",
    "        total_prediction += predictions.shape[0]\n",
    "\n",
    "      precision = 1.0 * correct_prediction / total_prediction\n",
    "      best_precision = max(precision, best_precision)\n",
    "\n",
    "      #precision_summ = tf.Summary()\n",
    "      #precision_summ.value.add(\n",
    "      #    tag='Precision', simple_value=precision)\n",
    "      #summary_writer.add_summary(precision_summ, train_step)\n",
    "      #best_precision_summ = tf.Summary()\n",
    "      #best_precision_summ.value.add(\n",
    "      #    tag='Best Precision', simple_value=best_precision)\n",
    "      #summary_writer.add_summary(best_precision_summ, train_step)\n",
    "      #summary_writer.add_summary(summaries, train_step)\n",
    "      tf.logging.info('precision: %.3f, best precision: %.3f' %\n",
    "                      (precision, best_precision))\n",
    "      break\n",
    "      #summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading checkpoint /tmp/cifar10_train/model.ckpt-10000\n",
      "INFO:tensorflow:precision: 0.464, best precision: 0.464\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
