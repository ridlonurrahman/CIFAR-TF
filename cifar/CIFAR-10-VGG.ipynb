{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10, cifar_input, cifar_input_ver2\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "'''tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10VGG_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 25000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")'''\n",
    "\n",
    "IMAGE_SIZE = 24\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "\n",
    "max_steps = 40000\n",
    "train_dir = 'cifar10VGG_train/'\n",
    "batch_size = 128\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "  \"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "  Add summary for \"Loss\" and \"Loss/avg\".\n",
    "  Args:\n",
    "    logits: Logits from inference().\n",
    "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "\n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  # Calculate the average cross entropy loss across the batch.\n",
    "  # labels = tf.cast(labels, tf.int64)\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "  # decay terms (L2 loss).\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "  \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "\n",
    "  Generates moving average for all losses and associated summaries for\n",
    "  visualizing the performance of the network.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "  Returns:\n",
    "    loss_averages_op: op for generating moving averages of losses.\n",
    "  \"\"\"\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "  losses = tf.get_collection('losses')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "  return loss_averages_op\n",
    "\n",
    "def train(total_loss, global_step):\n",
    "  \"\"\"Train CIFAR-10 model.\n",
    "\n",
    "  Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "  Returns:\n",
    "    train_op: op for training.\n",
    "  \"\"\"\n",
    "  # Variables that affect learning rate.\n",
    "  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / batch_size\n",
    "  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "  # Decay the learning rate exponentially based on the number of steps.\n",
    "  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "\n",
    "  # Generate moving averages of all losses and associated summaries.\n",
    "  loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "  # Compute gradients.\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "  # Apply gradients.\n",
    "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "  variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  \"\"\"Build the CIFAR-10 model.\n",
    "\n",
    "  Args:\n",
    "    images: Images returned from distorted_inputs() or inputs().\n",
    "\n",
    "  Returns:\n",
    "    Logits.\n",
    "  \"\"\"\n",
    "  # We instantiate all variables using tf.get_variable() instead of\n",
    "  # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "  # If we only ran this model on a single GPU, we could simplify this function\n",
    "  # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "  #\n",
    "  # conv1\n",
    "\n",
    "  # zero-mean input\n",
    "  # conv1_1\n",
    "  parameters = []\n",
    "  with tf.name_scope('conv1_1') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv1_1 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv1_2\n",
    "  with tf.name_scope('conv1_2') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv1_2 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # pool1\n",
    "  pool1 = tf.nn.max_pool(conv1_2,\n",
    "                         ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='SAME',\n",
    "                         name='pool1')\n",
    "\n",
    "  # conv2_1\n",
    "  with tf.name_scope('conv2_1') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv2_1 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv2_2\n",
    "  with tf.name_scope('conv2_2') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv2_2 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # pool2\n",
    "  pool2 = tf.nn.max_pool(conv2_2,\n",
    "                         ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='SAME',\n",
    "                         name='pool2')\n",
    "\n",
    "  # conv3_1\n",
    "  with tf.name_scope('conv3_1') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv3_1 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv3_2\n",
    "  with tf.name_scope('conv3_2') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv3_2 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv3_3\n",
    "  with tf.name_scope('conv3_3') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv3_3 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # pool3\n",
    "  pool3 = tf.nn.max_pool(conv3_3,\n",
    "                         ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='SAME',\n",
    "                         name='pool3')\n",
    "\n",
    "  # conv4_1\n",
    "  with tf.name_scope('conv4_1') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv4_1 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv4_2\n",
    "  with tf.name_scope('conv4_2') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv4_2 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv4_3\n",
    "  with tf.name_scope('conv4_3') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv4_3 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # pool4\n",
    "  pool4 = tf.nn.max_pool(conv4_3,\n",
    "                         ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='SAME',\n",
    "                         name='pool4')\n",
    "\n",
    "  # conv5_1\n",
    "  with tf.name_scope('conv5_1') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv5_1 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv5_2\n",
    "  with tf.name_scope('conv5_2') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv5_2 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # conv5_3\n",
    "  with tf.name_scope('conv5_3') as scope:\n",
    "      kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                               stddev=1e-1), name='weights')\n",
    "      conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "      biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      out = tf.nn.bias_add(conv, biases)\n",
    "      conv5_3 = tf.nn.relu(out, name=scope)\n",
    "      parameters += [kernel, biases]\n",
    "\n",
    "  # pool5\n",
    "  pool5 = tf.nn.max_pool(conv5_3,\n",
    "                         ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='SAME',\n",
    "                         name='pool4')\n",
    "\n",
    "  with tf.name_scope('fc1') as scope:\n",
    "      shape = int(np.prod(pool5.get_shape()[1:]))\n",
    "      fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                   dtype=tf.float32,\n",
    "                                                   stddev=1e-1), name='weights')\n",
    "      fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      pool5_flat = tf.reshape(pool5, [-1, shape])\n",
    "      fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "      fc1 = tf.nn.relu(fc1l)\n",
    "      parameters += [fc1w, fc1b]\n",
    "\n",
    "  # fc2\n",
    "  with tf.name_scope('fc2') as scope:\n",
    "      fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                   dtype=tf.float32,\n",
    "                                                   stddev=1e-1), name='weights')\n",
    "      fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      fc2l = tf.nn.bias_add(tf.matmul(fc1, fc2w), fc2b)\n",
    "      fc2 = tf.nn.relu(fc2l)\n",
    "      parameters += [fc2w, fc2b]\n",
    "\n",
    "  # fc3\n",
    "  with tf.name_scope('fc3') as scope:\n",
    "      fc3w = tf.Variable(tf.truncated_normal([4096, 10],\n",
    "                                                   dtype=tf.float32,\n",
    "                                                   stddev=1e-1), name='weights')\n",
    "      fc3b = tf.Variable(tf.constant(1.0, shape=[10], dtype=tf.float32),\n",
    "                           trainable=True, name='biases')\n",
    "      fc3l = tf.nn.bias_add(tf.matmul(fc2, fc3w), fc3b)\n",
    "      parameters += [fc3w, fc3b]\n",
    "\n",
    "  return fc3l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar_input.build_input('cifar10', 'cifar10/data_batch*', batch_size, 'train')\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Calculate losses.\n",
    "    losses = loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = train(losses, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs losses and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "        f = open(\"log.txt\",'ab')\n",
    "        f.write('\\n\\n==== Run ===\\nInfo: VGG\\n')\n",
    "        f.close()\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        self._start_time = time.time()\n",
    "        return tf.train.SessionRunArgs(losses)  # Asks for losses value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        duration = time.time() - self._start_time\n",
    "        losses_value = run_values.results\n",
    "        if self._step % 10 == 0:\n",
    "          num_examples_per_step = batch_size\n",
    "          examples_per_sec = num_examples_per_step / duration\n",
    "          sec_per_batch = float(duration)\n",
    "\n",
    "          format_str = ('%s: step %d, losses = %.3f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, losses_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "          f = open(\"log.txt\",'ab')\n",
    "          f.write('{0}: step {1}, losses = {2:.4f} ({3:.2f} examples/sec; {4:.2f} sec/batch)\\n'.format(datetime.now(), self._step, losses_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "          f.close()\n",
    "            \n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=train_dir,\n",
    "                                           hooks=[tf.train.StopAtStepHook(last_step=max_steps),\n",
    "                                                  tf.train.NanTensorHook(losses),\n",
    "                                                  _LoggerHook()],\n",
    "                                           config=tf.ConfigProto(\n",
    "                                               log_device_placement=log_device_placement)) as mon_sess:\n",
    "        while not mon_sess.should_stop():\n",
    "            mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "def evaluate():\n",
    "  eval_batch_count = 50\n",
    "  \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default() as g:\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar_input.build_input('cifar10', 'cifar10/test_batch.bin', 128, 'eval')\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "    saver = tf.train.Saver()\n",
    "    #summary_writer = tf.summary.FileWriter(FLAGS.eval_dir)\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    tf.train.start_queue_runners(sess)\n",
    "\n",
    "    best_precision = 0.0\n",
    "    try:\n",
    "      ckpt_state = tf.train.get_checkpoint_state(train_dir)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "      tf.logging.error('Cannot restore checkpoint: %s', e)\n",
    "    if not (ckpt_state and ckpt_state.model_checkpoint_path):\n",
    "      tf.logging.info('No model to eval yet at %s', train_dir)\n",
    "    tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "\n",
    "    total_prediction, correct_prediction = 0, 0\n",
    "    for _ in six.moves.range(eval_batch_count):\n",
    "      (predictions, truth) = sess.run(\n",
    "          [logits, labels])\n",
    "\n",
    "      truth = np.argmax(truth, axis=1)\n",
    "      predictions = np.argmax(predictions, axis=1)\n",
    "      correct_prediction += np.sum(truth == predictions)\n",
    "      total_prediction += predictions.shape[0]\n",
    "\n",
    "    precision = 1.0 * correct_prediction / total_prediction\n",
    "    best_precision = max(precision, best_precision)\n",
    "\n",
    "    tf.logging.info('precision: %.3f, best precision: %.3f' %\n",
    "                    (precision, best_precision))\n",
    "    f = open(\"log.txt\",'ab')\n",
    "    f.write('precision: {0}, best precision: {1}'.format(precision, best_precision))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
